{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract bz2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data export successful.\n",
      "Test data export successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_train='./data/train.ft.txt'\n",
    "\n",
    "with bz2.open('./data/train.ft.txt.bz2', 'rt', encoding='utf-8') as compressed_file, open(path_train, 'w', encoding='utf-8') as output_file:\n",
    "    for line in compressed_file:\n",
    "        output_file.write(line)\n",
    "    print('Train data export successful.')\n",
    "    \n",
    "path_test='./data/test.ft.txt'\n",
    "\n",
    "with bz2.open('./data/test.ft.txt.bz2', 'rt', encoding='utf-8') as compressed_file, open(path_test, 'w', encoding='utf-8') as output_file:\n",
    "    for line in compressed_file:\n",
    "        output_file.write(line)\n",
    "    print('Test data export successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "__label__2 The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\n",
      "__label__2 Amazing!: This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you've played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time's Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears soundtrack, so I can't say for sure), and even if you've never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.\n",
      "__label__2 Excellent Soundtrack: I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Unstealable Jewel.Overall, this is a excellent soundtrack and should be brought by those that like video game music.Xander Cross\n",
      "__label__2 Remember, Pull Your Jaw Off The Floor After Hearing it: If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of lines you want to display\n",
    "num_lines = 5  # Change this number as needed\n",
    "\n",
    "# Open the file and read the first 'num_lines' lines\n",
    "with open(path_train, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i < num_lines:\n",
    "            print(line.strip())  # Strip removes trailing newline characters\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3600000/3600000 [00:04<00:00, 761602.30it/s]\n",
      "100%|██████████| 400000/400000 [00:00<00:00, 842683.21it/s]\n"
     ]
    }
   ],
   "source": [
    "train,test,train_label,test_label=[],[],[],[]\n",
    "with open(path_train, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    train.append(line.split('__label__')[1][1:])\n",
    "    train_label.append(line.split('__label__')[1][0])\n",
    "with open(path_test, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    test.append(line.split('__label__')[1][1:])\n",
    "    test_label.append(line.split('__label__')[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random sample for faster processing\n",
    "import random\n",
    "seed = 123\n",
    "\n",
    "# Randomly select elements from the train list\n",
    "random.seed(seed)\n",
    "train_indices = random.sample(range(len(train)), 4000)\n",
    "train = [train[i] for i in train_indices]\n",
    "train_label = [train_label[i] for i in train_indices]\n",
    "\n",
    "# Randomly select elements from the test list\n",
    "random.seed(seed)\n",
    "test_indices = random.sample(range(len(test)), 200)\n",
    "test = [test[i] for i in test_indices]\n",
    "test_label = [test_label[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to pickle files\n",
    "with open('data/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "with open('data/train_label.pkl', 'wb') as f:\n",
    "    pickle.dump(train_label, f)\n",
    "with open('data/test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "with open('data/test_label.pkl', 'wb') as f:\n",
    "    pickle.dump(test_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle files\n",
    "with open('data/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('data/train_label.pkl', 'rb') as f:\n",
    "    train_label = pickle.load(f)\n",
    "with open('data/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "with open('data/test_label.pkl', 'rb') as f:\n",
    "    test_label = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert multiple whitespace characters to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length 4000\n",
      "Train Label Length 4000\n",
      "Test Length 200\n",
      "Test Label Length 200\n"
     ]
    }
   ],
   "source": [
    "print('Train Length',len(train))\n",
    "print('Train Label Length',len(train_label))\n",
    "print('Test Length',len(test))\n",
    "print('Test Label Length',len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2',\n",
       " ' A Different View of Blink 182: If you bought this CD after buying Enema of the state, like I did, youll be suprised at how much they have changed. This Cd has great songs and gives you another side of Blink-182. I like it.\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0],train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2',\n",
       " ' a different view of blink if you bought this cd after buying enema of the state like i did youll be suprised at how much they have changed this cd has great songs and gives you another side of blink i like it ')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0],clean_text(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a different view of blink if you bought this ...\n",
       "1     a holiday classic this was one of my favorite...\n",
       "2     np mistake do not buy this productthis price ...\n",
       "3     you may not get what you ordered i ordered yi...\n",
       "4     a great game that might have been the temple ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.DataFrame(train)[0].apply(clean_text)\n",
    "test=pd.DataFrame(test)[0].apply(clean_text)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords into a global variable\n",
    "# stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# # function to tokenize and remove stopwords from a single text\n",
    "# def remove_stopwords(series):\n",
    "#     return series.apply(lambda text: ' '.join(word for word in nltk.word_tokenize(text) if word.lower() not in stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove stopwords in parallel\n",
    "# def parallelize_tokenizer(df, func, n_cores=4):\n",
    "#     df_split = np.array_split(df, n_cores)\n",
    "#     pool = Parallel(n_jobs=n_cores)\n",
    "#     df = pd.concat(pool(delayed(func)(i) for i in df_split))\n",
    "#     return df\n",
    "\n",
    "# num_cores = joblib.cpu_count()\n",
    "# train_token = parallelize_tokenizer(train, remove_stopwords,num_cores)\n",
    "# test_token = parallelize_tokenizer(test, remove_stopwords,num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leocb\\OneDrive\\Documentos\\Projects\\ReviewsAnalysis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(text, max_len):\n",
    "    tokenized = tokenizer.encode_plus(\n",
    "        text, \n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"]\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "# Tokenize the sequences\n",
    "train_tokenized = [tokenize_and_pad(text, max_len) for text in train]\n",
    "test_tokenized = [tokenize_and_pad(text, max_len) for text in test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to pytorch files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_int = [int(t)-1 for t in train_label]\n",
    "test_label_int = [int(t)-1 for t in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert train_labels to a tensor\n",
    "train_labels_tensor = torch.tensor(train_label_int)\n",
    "test_labels_tensor = torch.tensor(test_label_int)\n",
    "\n",
    "# Save train_tokenized and train_labels_tensor\n",
    "torch.save((train_tokenized, train_labels_tensor), 'data/train_data.pt')\n",
    "torch.save((test_tokenized, test_labels_tensor), 'data/test_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the saved data\n",
    "train_data = torch.load('data/train_data.pt')\n",
    "test_data = torch.load('data/test_data.pt')\n",
    "\n",
    "# Separate the tokenized data and labels\n",
    "train_tokenized, train_labels_tensor = train_data\n",
    "test_tokenized, test_labels_tensor = test_data\n",
    "\n",
    "# Convert the tokenized data into tensors\n",
    "train_inputs = torch.stack([item[0] for item in train_tokenized]).squeeze()\n",
    "train_masks = torch.stack([item[1] for item in train_tokenized]).squeeze()\n",
    "test_inputs = torch.stack([item[0] for item in test_tokenized]).squeeze()\n",
    "test_masks = torch.stack([item[1] for item in test_tokenized]).squeeze()\n",
    "\n",
    "# Move everything to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train_inputs = train_inputs.to(device)\n",
    "train_masks = train_masks.to(device)\n",
    "train_labels_tensor = train_labels_tensor.to(device)\n",
    "test_inputs = test_inputs.to(device)\n",
    "test_masks = test_masks.to(device)\n",
    "test_labels_tensor = test_labels_tensor.to(device)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs, attention_mask=test_masks)\n",
    "    _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BertForSequenceClassification.from_pretrained('data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.535\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leocb\\OneDrive\\Documentos\\Projects\\ReviewsAnalysis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Move tensors back to CPU for metric calculation\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "test_labels_np = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels_np, predicted_labels_np)\n",
    "precision = precision_score(test_labels_np, predicted_labels_np)\n",
    "recall = recall_score(test_labels_np, predicted_labels_np)\n",
    "f1 = f1_score(test_labels_np, predicted_labels_np)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
