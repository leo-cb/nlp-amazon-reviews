{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract bz2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_train='./data/train.ft.txt'\n",
    "\n",
    "with bz2.open('./data/train.ft.txt.bz2', 'rt', encoding='utf-8') as compressed_file, open(path_train, 'w', encoding='utf-8') as output_file:\n",
    "    for line in compressed_file:\n",
    "        output_file.write(line)\n",
    "    print('Train data export successful.')\n",
    "    \n",
    "path_test='./data/test.ft.txt'\n",
    "\n",
    "with bz2.open('./data/test.ft.txt.bz2', 'rt', encoding='utf-8') as compressed_file, open(path_test, 'w', encoding='utf-8') as output_file:\n",
    "    for line in compressed_file:\n",
    "        output_file.write(line)\n",
    "    print('Test data export successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of lines you want to display\n",
    "num_lines = 5  # Change this number as needed\n",
    "\n",
    "# Open the file and read the first 'num_lines' lines\n",
    "with open(path_train, 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i < num_lines:\n",
    "            print(line.strip())  # Strip removes trailing newline characters\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test,train_label,test_label=[],[],[],[]\n",
    "with open(path_train, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    train.append(line.split('__label__')[1][1:])\n",
    "    train_label.append(line.split('__label__')[1][0])\n",
    "with open(path_test, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "for line in tqdm(lines):\n",
    "    test.append(line.split('__label__')[1][1:])\n",
    "    test_label.append(line.split('__label__')[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random sample for faster processing\n",
    "import random\n",
    "seed = 123\n",
    "\n",
    "# Randomly select elements from the train list\n",
    "random.seed(seed)\n",
    "train_indices = random.sample(range(len(train)), 4000)\n",
    "train = [train[i] for i in train_indices]\n",
    "train_label = [train_label[i] for i in train_indices]\n",
    "\n",
    "# Randomly select elements from the test list\n",
    "random.seed(seed)\n",
    "test_indices = random.sample(range(len(test)), 200)\n",
    "test = [test[i] for i in test_indices]\n",
    "test_label = [test_label[i] for i in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to pickle files\n",
    "with open('data/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "with open('data/train_label.pkl', 'wb') as f:\n",
    "    pickle.dump(train_label, f)\n",
    "with open('data/test.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "with open('data/test_label.pkl', 'wb') as f:\n",
    "    pickle.dump(test_label, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from pickle files\n",
    "with open('data/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('data/train_label.pkl', 'rb') as f:\n",
    "    train_label = pickle.load(f)\n",
    "with open('data/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "with open('data/test_label.pkl', 'rb') as f:\n",
    "    test_label = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert multiple whitespace characters to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Length',len(train))\n",
    "print('Train Label Length',len(train_label))\n",
    "print('Test Length',len(test))\n",
    "print('Test Label Length',len(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label[0],train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label[0],clean_text(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(train)[0].apply(clean_text)\n",
    "test=pd.DataFrame(test)[0].apply(clean_text)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize_and_pad(text, max_len):\n",
    "    tokenized = tokenizer.encode_plus(\n",
    "        text, \n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"]\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "# Tokenize the sequences\n",
    "train_tokenized = [tokenize_and_pad(text, max_len) for text in train]\n",
    "test_tokenized = [tokenize_and_pad(text, max_len) for text in test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to pytorch files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_int = [int(t)-1 for t in train_label]\n",
    "test_label_int = [int(t)-1 for t in test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert train_labels to a tensor\n",
    "train_labels_tensor = torch.tensor(train_label_int)\n",
    "test_labels_tensor = torch.tensor(test_label_int)\n",
    "\n",
    "# Save train_tokenized and train_labels_tensor\n",
    "torch.save((train_tokenized, train_labels_tensor), 'data/train_data.pt')\n",
    "torch.save((test_tokenized, test_labels_tensor), 'data/test_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the saved data\n",
    "train_data = torch.load('data/train_data.pt')\n",
    "test_data = torch.load('data/test_data.pt')\n",
    "\n",
    "# Separate the tokenized data and labels\n",
    "train_tokenized, train_labels_tensor = train_data\n",
    "test_tokenized, test_labels_tensor = test_data\n",
    "\n",
    "# Convert the tokenized data into tensors\n",
    "train_inputs = torch.stack([item[0] for item in train_tokenized]).squeeze()\n",
    "train_masks = torch.stack([item[1] for item in train_tokenized]).squeeze()\n",
    "test_inputs = torch.stack([item[0] for item in test_tokenized]).squeeze()\n",
    "test_masks = torch.stack([item[1] for item in test_tokenized]).squeeze()\n",
    "\n",
    "# Move everything to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train_inputs = train_inputs.to(device)\n",
    "train_masks = train_masks.to(device)\n",
    "train_labels_tensor = train_labels_tensor.to(device)\n",
    "test_inputs = test_inputs.to(device)\n",
    "test_masks = test_masks.to(device)\n",
    "test_labels_tensor = test_labels_tensor.to(device)\n",
    "\n",
    "# Predict the labels for the test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_inputs, attention_mask=test_masks)\n",
    "    _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = BertForSequenceClassification.from_pretrained('data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Move tensors back to CPU for metric calculation\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "test_labels_np = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels_np, predicted_labels_np)\n",
    "precision = precision_score(test_labels_np, predicted_labels_np)\n",
    "recall = recall_score(test_labels_np, predicted_labels_np)\n",
    "f1 = f1_score(test_labels_np, predicted_labels_np)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
